<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>After effect of Postgres DB upgrade | Software Engineering - Software hacks in small packs</title>
<meta name=keywords content="docker,postgres,postgresql,programming"><meta name=description content="The Story:


Client reported that one of the reports which was getting generated in 15-20 mins is not taking ours.


We tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.


Recently we upgraded the Postgres version from 12 to 15 on two of our projects.


We had two possible causes(still a theory)


Recently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause."><meta name=author content><link rel=canonical href=https://rupeshsharma.in/posts/after-effect-of-postgres-db-upgrade/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://rupeshsharma.in/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rupeshsharma.in/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rupeshsharma.in/favicon-32x32.png><link rel=apple-touch-icon href=https://rupeshsharma.in/apple-touch-icon.png><link rel=mask-icon href=https://rupeshsharma.in/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://rupeshsharma.in/posts/after-effect-of-postgres-db-upgrade/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://rupeshsharma.in/posts/after-effect-of-postgres-db-upgrade/"><meta property="og:site_name" content="Software Engineering - Software hacks in small packs"><meta property="og:title" content="After effect of Postgres DB upgrade"><meta property="og:description" content="The Story:
Client reported that one of the reports which was getting generated in 15-20 mins is not taking ours.
We tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.
Recently we upgraded the Postgres version from 12 to 15 on two of our projects.
We had two possible causes(still a theory)
Recently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-24T00:00:00+00:00"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Postgres"><meta property="article:tag" content="Postgresql"><meta property="article:tag" content="Programming"><meta name=twitter:card content="summary"><meta name=twitter:title content="After effect of Postgres DB upgrade"><meta name=twitter:description content="The Story:


Client reported that one of the reports which was getting generated in 15-20 mins is not taking ours.


We tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.


Recently we upgraded the Postgres version from 12 to 15 on two of our projects.


We had two possible causes(still a theory)


Recently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rupeshsharma.in/posts/"},{"@type":"ListItem","position":2,"name":"After effect of Postgres DB upgrade","item":"https://rupeshsharma.in/posts/after-effect-of-postgres-db-upgrade/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"After effect of Postgres DB upgrade","name":"After effect of Postgres DB upgrade","description":"The Story:\nClient reported that one of the reports which was getting generated in 15-20 mins is not taking ours.\nWe tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.\nRecently we upgraded the Postgres version from 12 to 15 on two of our projects.\nWe had two possible causes(still a theory)\nRecently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause.\n","keywords":["docker","postgres","postgresql","programming"],"articleBody":"The Story:\nClient reported that one of the reports which was getting generated in 15-20 mins is not taking ours.\nWe tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.\nRecently we upgraded the Postgres version from 12 to 15 on two of our projects.\nWe had two possible causes(still a theory)\nRecently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause.\nClient ingested more data into the system which caused query performance to degrade.\nSo I turned to looking at shared buffer memory size and thought it can be optimised.\nBut we were not sure whether it was a good idea or not. That’s why we got into call with the AWS team.\nDuring the call realised its already set to its maximum value.Then told AWS about the recent DB version upgrade.\nThen AWS told us about a few post upgrade activities to be done.\nWe did those and Voila the query again started giving the same performance.\nAbout the post DB upgrade Activity:\nPost DB migration we are supposed to run the ANALYZE VERBOSE; command so that PG updates the table statistics and the query planner can use those statistics to determine the most efficient query plan.\nReferences:\nhttps://aws.amazon.com/blogs/database/amazon-aurora-postgresql-parameters-part-1-memory-and-query-plan-management/https://www.postgresql.org/docs/10/sql-analyze.html\n","wordCount":"217","inLanguage":"en","datePublished":"2024-12-24T00:00:00Z","dateModified":"2024-12-24T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://rupeshsharma.in/posts/after-effect-of-postgres-db-upgrade/"},"publisher":{"@type":"Organization","name":"Software Engineering - Software hacks in small packs","logo":{"@type":"ImageObject","url":"https://rupeshsharma.in/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://rupeshsharma.in/ accesskey=h title="Software Engineering - Software hacks in small packs (Alt + H)">Software Engineering - Software hacks in small packs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">After effect of Postgres DB upgrade</h1><div class=post-meta><span title='2024-12-24 00:00:00 +0000 UTC'>December 24, 2024</span></div></header><div class=post-content><p><strong>The Story:</strong></p><ul><li><p>Client reported that one of the reports which was getting generated in 15-20 mins is not taking ours.</p></li><li><p>We tried analysing the RDS performance insight and came to know IO:DataFileRead is taking time.</p></li><li><p>Recently we upgraded the Postgres version from 12 to 15 on two of our projects.</p></li><li><p>We had two possible causes(still a theory)</p><ul><li><p>Recently we upgraded the Postgres version from 12 to 15 on two of our projects. It can be the root cause.</p></li><li><p>Client ingested more data into the system which caused query performance to degrade.</p></li></ul></li><li><p>So I turned to looking at shared buffer memory size and thought it can be optimised.</p></li><li><p>But we were not sure whether it was a good idea or not. That&rsquo;s why we got into call with the AWS team.</p></li><li><p>During the call realised its already set to its maximum value.Then told AWS about the recent DB version upgrade.</p></li><li><p>Then AWS told us about a few post upgrade activities to be done.</p></li><li><p>We did those and Voila<img alt=:tada: loading=lazy src=/posts/after-effect-of-postgres-db-upgrade/images/AD_4nXf3DMGZM-TB3f-FZILrqp3HICI3Rk7GcakaG5AGU9W1LjgUtIelmfDVBN9pfV4EwZlU77_-13GraoRKnDKjzD3dIxGGq9vm0Kc_sK0ud6s833HeyMsTroQotk0DemJKfGwpX1HKBprcAEqpoPvjO0yO6Wgc> the query again started giving the same performance.</p></li></ul><p><strong>About the post DB upgrade Activity:</strong></p><ul><li><p>Post DB migration we are supposed to run the ANALYZE VERBOSE; command so that PG updates the table statistics and the query planner can use those statistics to determine the most efficient query plan.</p></li><li><p>References:</p></li></ul><p><a href=https://aws.amazon.com/blogs/database/amazon-aurora-postgresql-parameters-part-1-memory-and-query-plan-management/>https://aws.amazon.com/blogs/database/amazon-aurora-postgresql-parameters-part-1-memory-and-query-plan-management/</a><a href=https://www.postgresql.org/docs/10/sql-analyze.html>https://www.postgresql.org/docs/10/sql-analyze.html</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://rupeshsharma.in/tags/docker/>Docker</a></li><li><a href=https://rupeshsharma.in/tags/postgres/>Postgres</a></li><li><a href=https://rupeshsharma.in/tags/postgresql/>Postgresql</a></li><li><a href=https://rupeshsharma.in/tags/programming/>Programming</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://rupeshsharma.in/>Software Engineering - Software hacks in small packs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>